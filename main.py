#!/usr/env python3
import glob
import json
import os

import numpy as np
import torch
import torch.nn.functional as functional
from matplotlib.colors import LinearSegmentedColormap
from PIL import Image
from torchvision import models, transforms
from tqdm.auto import tqdm

from captum.attr import IntegratedGradients, NoiseTunnel
from captum.attr import visualization as viz


def captum_on_image(
    image_path: str,
    model: torch.nn.Module,
    idx_to_labels: dict,
) -> None:
    """Runs captum explanations on image."""

    # create the transformation pipeline
    transform = transforms.Compose(
        [
            transforms.Resize(256),
            transforms.CenterCrop(224),
            transforms.ToTensor(),
        ]
    )
    transform_normalize = transforms.Normalize(
        mean=[0.485, 0.456, 0.406],
        std=[0.229, 0.224, 0.225],
    )

    # open image file
    img = Image.open(image_path)
    transformed_img = transform(img)

    # convert image to tensor
    model_input = transform_normalize(transformed_img)
    model_input = model_input.unsqueeze(0)

    # run the model on the input image
    model_output = model(model_input)
    output = functional.softmax(input=model_output, dim=1)
    prediction_score, pred_label_idx = torch.topk(input=output, k=1)

    # get the label of the predicted class
    pred_label_idx.squeeze_()
    predicted_label = idx_to_labels[str(pred_label_idx.item())][1]
    tqdm.write(
        "Predicted: {} ({:.2f}%)".format(
            predicted_label, 100 * prediction_score.squeeze().item()
        )
    )

    # create the gradient-based attributions
    integrated_gradients = IntegratedGradients(model)
    attributions_ig = integrated_gradients.attribute(
        inputs=model_input, target=pred_label_idx, n_steps=200
    )

    fig_size = (12, 6)

    # visualize the image and attributions by overlaying the attributions on the image
    grayscale_cmap = LinearSegmentedColormap.from_list(
        "custom blue", [(0, "#ffffff"), (0.25, "#000000"), (1, "#000000")], N=256
    )
    # _ = viz.visualize_image_attr(
    #     np.transpose(attributions_ig.squeeze().cpu().detach().numpy(), (1, 2, 0)),
    #     np.transpose(transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)),
    #     method="heat_map",
    #     cmap=grayscale_cmap,
    #     show_colorbar=True,
    #     sign="positive",
    #     fig_size=fig_size,
    #     outlier_perc=1,
    # )

    # compute the attributions using integrated gradients and
    # smooth them across multiple images generated by a noise tunnel
    noise_tunnel = NoiseTunnel(integrated_gradients)
    attributions_ig_nt = noise_tunnel.attribute(
        inputs=model_input,
        nt_samples=10,
        nt_type="smoothgrad_sq",
        target=pred_label_idx,
    )
    fig, _ = viz.visualize_image_attr_multiple(
        attr=np.transpose(
            attributions_ig_nt.squeeze().cpu().detach().numpy(), (1, 2, 0)
        ),
        original_image=np.transpose(
            transformed_img.squeeze().cpu().detach().numpy(), (1, 2, 0)
        ),
        methods=["original_image", "heat_map"],
        signs=["all", "positive"],
        cmap="viridis",
        fig_size=fig_size,
        show_colorbar=True,
    )
    fig.tight_layout()
    fig.patch.set_facecolor("none")
    output_dir = "examples"
    fname = "{}_ig_nt.png".format(os.path.basename(image_path).split(".")[0])
    os.makedirs(output_dir, exist_ok=True)
    fig.savefig(os.path.join(output_dir, fname))
    tqdm.write("Saved to {}".format(os.path.join(output_dir, fname)))


def main():

    # download the json with imagenet labels if it doesn't exist
    labels_path = os.path.join("models", "imagenet_class_index.json")
    json_src = "https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json"
    if not os.path.exists(labels_path):
        os.system("wget -P models {}".format(json_src))
    with open(labels_path) as json_data:
        idx_to_labels = json.load(json_data)

    # loads the pretrained model
    model = models.resnet18(pretrained=True)
    model = model.eval()

    # for each image in folder, run captum
    for image_path in tqdm(glob.glob("img/*.jpg"), ncols=80, unit="img", colour="blue"):
        captum_on_image(image_path, model=model, idx_to_labels=idx_to_labels)


if __name__ == "__main__":
    main()
